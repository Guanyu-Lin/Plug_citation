[train] #train parameters
epoch = 16
batch_size = 16

shuffle = True

reader_num = 1

optimizer = AdamW
learning_rate = 2e-7
weight_decay = 1e-6

multilabel = False
warmup_steps=20
training_steps=60
max_grad_norm=1.0

ignore_no_grad=False

save_step=-1

[eval] #eval parameters

shuffle = False
reader_num = 1

[distributed]
use = True
backend = nccl

[data] #data parameters


[model] #model parameters
pretrained_model_path = ../LLaMA-2-7b-chat-modelcenter

adapter_path = None

[output] #output parameters
output_time = 10
test_time = 1
output_grad_step=200

model_path = checkpoint

output_function = squad
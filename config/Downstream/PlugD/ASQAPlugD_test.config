[train] #train parameters
epoch = 2
batch_size = 8 

shuffle = True
no_valid = False
valid_mode = batch
reader_num = 4

optimizer = AdamW
learning_rate = 1e-4
weight_decay = 1e-5
grad_accumulate = 10

warmup_steps=30
training_steps=119
max_grad_norm=1.0

scheduler=cosine

inspector_para=*plug*

max_len = 1024
ctx_len = 1024
ans_max_len = 1024

layerth = 30
bottleneck_dim=16


[eval] #eval parameters
batch_size = 8
reader_num = 4

[data] #data parameters

train_dataset_type = ASQA
train_formatter_type = ASQAPlugD
train_data_path = ../ALCE/data/asqa_eval_gtr_top100.json


prompt_question_data_path: ../ALCE/prompts/asqa_default_question.json
prompt_question_ctxs_data_path: ../ALCE/prompts/asqa_default.json
prompt_ctxs_data_path: ../ALCE/prompts/asqa_default_ctxs.json

valid_dataset_type = ASQA
valid_formatter_type = ASQAPlugD
valid_data_path = ../ALCE/data/asqa_eval_gtr_top100.json



[model] #model parameters
model_name = Seq2Seq
pretrained_model = llama

model_type = PlugD

[output] #output parameters
output_time = 10
test_time = 1
output_grad_step = 200

model_name = ASQAPlugD
output_function = squad

output_grad = False
